{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Boosting performance on regresssion",
      "provenance": [],
      "authorship_tag": "ABX9TyPDjU5x7+uEEi9wMp8ugYN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurraSujay/MachineLearning-Projects/blob/main/Gradient_Boosting_performance_on_regresssion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIg5A2QgOwaj"
      },
      "source": [
        "#**Gradient Boosting in Python**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Offto3lCPC-F"
      },
      "source": [
        "compareing the performance of **AdaBoost and Gradient boosting** on a regression problem. Here we specifically use the diabetes dataset from the sk-learn library to compare the two algorithms. The dataset contains ten baseline variables, i.e., age, sex, body mass index, average blood pressure, and six blood serum measurements that were obtained for 442 diabetes patients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkbyKVxAHdaD",
        "outputId": "4e8148fb-ebe9-409b-9055-4aa4d2cc1b81"
      },
      "source": [
        "from sklearn import datasets, ensemble\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "diabetes = datasets.load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=13)\n",
        "\n",
        "params = {'n_estimators': 500,\n",
        "          'max_depth': 4,\n",
        "          'min_samples_split': 5,\n",
        "          'learning_rate': 0.01,\n",
        "          'loss': 'ls'}\n",
        "#gradient boosting classifier\n",
        "reg = ensemble.GradientBoostingRegressor(**params)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#adaboost classifier\n",
        "reg1=ensemble.AdaBoostRegressor()\n",
        "reg1.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
        "print(\"The mean squared error (MSE) on test set for gradient boosting: {:.4f}\".format(mse))\n",
        "\n",
        "mse1 = mean_squared_error(y_test, reg1.predict(X_test))\n",
        "print(\"The mean squared error (MSE) on test set for adaboost : {:.4f}\".format(mse1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean squared error (MSE) on test set for gradient boosting: 3050.4351\n",
            "The mean squared error (MSE) on test set for adaboost : 3203.8363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXpM8S2qOd5-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}